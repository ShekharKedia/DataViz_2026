# Another option: coercing hsgrad from a character vector to a logical vector
as.logical(as.numeric(as.factor(graduation$hsgrad))-1)
graduation <- read.table("http://statmath.wu.ac.at/courses/StatsWithR/Powers.txt")
saveRDS(graduation, "data/dat")
as.numeric(as.factor(graduation$hsgrad)
)
graduation$hsgrad
ifelse(graduation$hsgrad == "yes", 1, 0)
ifelse(graduation$hsgrad == "Yes", 1, 0)
# Or using ifelse()
as.logical(ifelse(graduation$hsgrad == "Yes", 1, 0))
## Reading in the data
#  Option 1:
#  Using stringsAsFactors
graduation <- read.table("http://statmath.wu.ac.at/courses/StatsWithR/Powers.txt",
stringsAsFactors = TRUE)
## a) Run the logit regression
mod <- glm(hsgrad ~ ., # period functions as omnibus selector (kitchen sink additive model)
data = graduation,
family = "binomial")
summary(mod)
## Likelihood ratio test
#  Create a null model
nullMod <- glm(as.factor(hsgrad) ~ 1, # 1 = fit an intercept only (i.e. sort of a "mean")
data = graduation,
family = "binomial")
#  Run an anova test on the model compared to the null model
anova(nullMod, mod, test = "Chisq")
summary(nullMod)
class(graduation$hsgrad)
## Likelihood ratio test
#  Create a null model
nullMod <- glm(hsgrad ~ 1, # 1 = fit an intercept only (i.e. sort of a "mean")
data = graduation,
family = "binomial")
#  Run an anova test on the model compared to the null model
anova(nullMod, mod, test = "Chisq")
?anova
anova(nullMod, mod, test = "LRT")
exp(confint(mod)) # Remember: transform to odds ratio using exp()
# An option for making a data.frame of confidence intervals and coefficients
confMod <- data.frame(cbind(lower = exp(confint(mod)[,1]),
coefs = exp(coef(mod)),
upper = exp(confint(mod)[,2])))
# Then use this to make a plot
ggplot(data = confMod, mapping = aes(x = row.names(confMod), y = coefs)) +
geom_point() +
geom_errorbar(aes(ymin = lower, ymax = upper), colour = "red") +
coord_flip()
## Load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
lapply(c("tidyverse"),  pkgTest)
# Then use this to make a plot
ggplot(data = confMod, mapping = aes(x = row.names(confMod), y = coefs)) +
geom_point() +
geom_errorbar(aes(ymin = lower, ymax = upper), colour = "red") +
coord_flip()
# Then use this to make a plot
ggplot(data = confMod, mapping = aes(x = row.names(confMod), y = coefs)) +
geom_point() +
geom_errorbar(aes(ymin = lower, ymax = upper), colour = "red") +
coord_flip() +
labs(x = "Terms", y = "Coefficients")
model.matrix( ~ unique(nsibs), data = graduation)
model.matrix( ~ as.factor(unique(nsibs)), data = graduation)
# As a side note, we can use unique() with model.matrix() to create a matrix
# of different combinations of factor levels to use with predict(). Another
# function that can help with this is expand.grid()
with(graduation, expand.grid(nonwhite = unique(nonwhite),
mhs = unique(mhs),
fhs = unique(fhs)))
unique(graduation$nsibs)
# A better function to help with this is expand.grid()
with(graduation, expand.grid(nonwhite = unique(nonwhite),
mhs = unique(mhs),
fhs = unique(fhs)))
# Consider for instance if we had a model just consisting of factors:
mod2 <- glm(hsgrad ~ nonwhite + mhs + fhs,
data = graduation,
family = "binomial")
predicted_data <- with(graduation, expand.grid(nonwhite = unique(nonwhite),
mhs = unique(mhs),
fhs = unique(fhs)))
predicted_data <- cbind(predicted_data, predict(mod2,
newdata = predicted_data,
type = "response",
se = TRUE))
# Now we can use the code in Jeff's lecture to fill out the confidence intervals
# and predicted probability (see lecture)
predicted_data <- within(predicted_data,
{PredictedProb <- plogis(fit)
LL <- plogis(fit - (1.96 * se.fit))
UL <- plogis(fit + (1.96 * se.fit))
})
predicted_data
graduation$nsibs_cut <- cut(graduation$nsibs,
breaks = c(0, 0.9, 1, 3, Inf),
include.lowest = TRUE,
labels = c("None", "One", "Two_Three", "FourPlus"))
mod3 <- glm(hsgrad ~.,
data = graduation[,!names(graduation) %in% c("nsibs", "nsibs_f")],
family = "binomial")
summary(mod3)
summary(mod)
# Extract confidence intervals around the estimates
confMod3 <- data.frame(cbind(lower = exp(confint(mod3)[,1]),
coefs = exp(coef(mod3)),
upper = exp(confint(mod3)[,2])))
# Plot the estimates and confidence intervals
ggplot(data = confMod3, mapping = aes(x = row.names(confMod3), y = coefs)) +
geom_point() +
geom_errorbar(aes(ymin = lower, ymax = upper), colour = "red") +
coord_flip()
# Plot the estimates and confidence intervals
ggplot(data = confMod3, mapping = aes(x = row.names(confMod3), y = coefs)) +
geom_point() +
geom_errorbar(aes(ymin = lower, ymax = upper), colour = "red") +
coord_flip() +
scale_y_continuous(breaks = seq(0,8,1)) +
labs(x = "Terms", y = "Coefficients")
?confint
summary(mod3)
?glm
## Load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
lapply(c("tidyverse"),  pkgTest)
## Reading in the data
#  Option 1:
#  Using stringsAsFactors
graduation <- read.table("http://statmath.wu.ac.at/courses/StatsWithR/Powers.txt",
stringsAsFactors = TRUE)
#  Option 3:
#  Coerce from a character vector to a logical vector
graduation$hsgrad <- as.logical(as.numeric(as.factor(graduation$hsgrad))-1)
## a) Run the logit regression
mod <- glm(hsgrad ~ ., # period functions as omnibus selector (kitchen sink additive model)
data = graduation,
family = "binomial")
?binomial
## Likelihood ratio test
#  Create a null model
nullMod <- glm(hsgrad ~ 1, # 1 = fit an intercept only (i.e. sort of a "mean")
data = graduation,
family = "binomial")
#  Run an anova test on the model compared to the null model
anova(nullMod, mod, test = "Chisq")
exp(confint(mod)) # Remember: transform to odds ratio using exp()
# An option for making a data.frame of confidence intervals and coefficients
confMod <- data.frame(cbind(lower = exp(confint(mod)[,1]),
coefs = exp(coef(mod)),
upper = exp(confint(mod)[,2])))
head(confMod)
# Then use this to make a plot
ggplot(data = confMod, mapping = aes(x = row.names(confMod), y = coefs)) +
geom_point() +
geom_errorbar(aes(ymin = lower, ymax = upper), colour = "red") +
coord_flip() +
labs(x = "Terms", y = "Coefficients")
range(graduation$nsibs)
summary(graduation$nsibs)
hist(graduation$nsibs)
which(graduation$nsibs > 0)
which(graduation$nsibs < 0)
graduation <- graduation[-which(graduation$nsibs < 0),]
## Reading in the data
#  Option 1:
#  Using stringsAsFactors
graduation <- read.table("http://statmath.wu.ac.at/courses/StatsWithR/Powers.txt",
stringsAsFactors = TRUE)
# Drop problematic cases
graduation <- graduation[-which(graduation$nsibs < 0),]
#  Option 3:
#  Coerce from a character vector to a logical vector
graduation$hsgrad <- as.logical(as.numeric(as.factor(graduation$hsgrad))-1)
## a) Run the logit regression
mod <- glm(hsgrad ~ ., # period functions as omnibus selector (kitchen sink additive model)
data = graduation,
family = "binomial")
## Likelihood ratio test
#  Create a null model
nullMod <- glm(hsgrad ~ 1, # 1 = fit an intercept only (i.e. sort of a "mean")
data = graduation,
family = "binomial")
#  Run an anova test on the model compared to the null model
anova(nullMod, mod, test = "Chisq")
# An option for making a data.frame of confidence intervals and coefficients
confMod <- data.frame(cbind(lower = exp(confint(mod)[,1]),
coefs = exp(coef(mod)),
upper = exp(confint(mod)[,2])))
# Then use this to make a plot
ggplot(data = confMod, mapping = aes(x = row.names(confMod), y = coefs)) +
geom_point() +
geom_errorbar(aes(ymin = lower, ymax = upper), colour = "red") +
coord_flip() +
labs(x = "Terms", y = "Coefficients")
model.matrix( ~ unique(nsibs), data = graduation) # I see a problem with the data here...
# As a side note, we can use unique() with model.matrix() to create a matrix
# of different combinations of factor levels to use with predict(). Though it's
# probably not the best approach...
model.matrix( ~ as.factor(unique(nsibs)), data = graduation)
# A better function to help with this is expand.grid()
with(graduation, expand.grid(nonwhite = unique(nonwhite),
mhs = unique(mhs),
fhs = unique(fhs)))
# Consider for instance if we had a model just consisting of factors:
mod2 <- glm(hsgrad ~ nonwhite + mhs + fhs,
data = graduation,
family = "binomial")
predicted_data <- with(graduation, expand.grid(nonwhite = unique(nonwhite),
mhs = unique(mhs),
fhs = unique(fhs)))
predicted_data <- cbind(predicted_data, predict(mod2,
newdata = predicted_data,
type = "response",
se = TRUE))
# Now we can use the code in Jeff's lecture to fill out the confidence intervals
# and predicted probability (see lecture)
predicted_data <- within(predicted_data,
{PredictedProb <- plogis(fit)
LL <- plogis(fit - (1.96 * se.fit))
UL <- plogis(fit + (1.96 * se.fit))
})
predicted_data
graduation$nsibs_cut <- cut(graduation$nsibs,
breaks = c(0, 0.9, 1, 3, Inf),
include.lowest = TRUE,
labels = c("None", "One", "Two_Three", "FourPlus"))
mod3 <- glm(hsgrad ~.,
data = graduation[,!names(graduation) %in% c("nsibs", "nsibs_f")],
family = "binomial")
summary(mod3)
# Extract confidence intervals around the estimates
confMod3 <- data.frame(cbind(lower = exp(confint(mod3)[,1]),
coefs = exp(coef(mod3)),
upper = exp(confint(mod3)[,2])))
# Plot the estimates and confidence intervals
ggplot(data = confMod3, mapping = aes(x = row.names(confMod3), y = coefs)) +
geom_point() +
geom_errorbar(aes(ymin = lower, ymax = upper), colour = "red") +
coord_flip() +
scale_y_continuous(breaks = seq(0,8,1)) +
labs(x = "Terms", y = "Coefficients")
?coef
# Download US trade data with Ireland for 2018-2020
# table: yrpc
# reporters: USA
us_trade <- ots_create_tidy_data(
years = 2018:2020,
reporters = "USA",
table = "yrpc"
)
# Remove objects
rm(list=ls())
# Detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats", "package:graphics", "package:grDevices", "package:utils", "package:datasets", "package:methods", "package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:", search()))==1, TRUE, FALSE)]
package.list <- setdiff(package.list, basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package,  character.only=TRUE)
}
detachAllPackages()
# Load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
# Load any necessary packages
lapply(c("tidyverse", "ggplot2", "ggridges", "tradestatistics", "dplyr"),  pkgTest)
# Set working directory for current folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
getwd()
# Download US trade data with Ireland for 2018-2020
# table: yrpc
# reporters: USA
us_trade <- ots_create_tidy_data(
years = 2018:2020,
reporters = "USA",
table = "yrpc"
)
us_imports_chn <- us_trade %>%
filter(
partner_iso == "CHN",
year %in% 2018:2020
)
us_imports_chn <- us_imports_chn %>%
mutate(
log_import = log(trade_value_usd_imp)
)
us_trade <- us_trade %>%
mutate(
log_import = log(trade_value_usd_imp)
)
# For 2020 US imports:
# What are the top 5 import partners by total value
top5_partners_2020 <- us_trade %>%
filter(year == 2020) %>%
group_by(partner_name) %>%
summarize(
total_imports = sum(trade_value_usd_imp, na.rm = TRUE),
.groups = "drop"
) %>%
arrange(desc(total_imports)) %>%
slice_head(n = 5)
View(top5_partners_2020)
View(us_imports_chn)
### Prepare HS2 code
us_trade <- us_trade %>%
mutate(
hs2 = substr(commodity_code, 1, 2)
)
### Compute averages
avg_import_hs2 <- us_trade %>%
filter(
year == 2020,
partner_name %in% top5_partners_2020$partner_name[1:3]
) %>%
group_by(partner_name, hs2) %>%
summarize(
avg_import = mean(trade_value_usd_imp, na.rm = TRUE),
.groups = "drop"
)
View(avg_import_hs2)
growth_partner <- us_trade %>%
filter(year %in% c(2018, 2020)) %>%
group_by(partner_name, year) %>%
summarize(
total_imports = sum(trade_value_usd_imp, na.rm = TRUE),
.groups = "drop"
) %>%
tidyr::pivot_wider(
names_from = year,
values_from = total_imports
) %>%
mutate(
growth_rate = (`2020` - `2018`) / `2018`
)
View(growth_partner)
# Let's investigate electronics growth
# Filter chapter_code = 85 (Machinery and mechanical appliances...) imports
electronics <- us_trade %>%
filter(
chapter_code == "85",
trade_value_usd_imp > 0
)
# Calculate year-over-year growth
electronics_growth <- electronics %>%
group_by(partner_name, year) %>%
summarize(
total_imports = sum(trade_value_usd_imp, na.rm = TRUE),
.groups = "drop"
) %>%
arrange(partner_name, year) %>%
group_by(partner_name) %>%
mutate(
yoy_growth = (total_imports - lag(total_imports)) / lag(total_imports)
)
View(electronics_growth)
# Rank partners by 2020 value AND growth rate
electronics_rank <- electronics_growth %>%
filter(year == 2020)  %>%
group_by(partner_name) %>%
arrange(desc(total_imports), desc(yoy_growth))
View(electronics_rank)
View(electronics_growth)
# Select & arrange top 10
electronics_rank_10 <- electronics_growth %>%
filter(year == 2020) %>%
ungroup() %>%
arrange(desc(total_imports), desc(yoy_growth)) %>%
slice_head(n = 10)
View(electronics_rank_10)
# Visualize 2020 US import values:
# Histogram with default bins vs binwidth = 1e9, boundary = 0
ggplot(us_trade %>% filter(year == 2020),
aes(trade_value_usd_imp)) +
geom_histogram() +
labs(title = "Default histogram (30 bins)")
ggplot(us_trade %>% filter(year == 2020),
aes(trade_value_usd_imp)) +
geom_histogram(binwidth = 1e9, boundary = 0) +
labs(title = "Histogram with binwidth = 1e9")
# Log-scale version of best histogram
ggplot(us_trade %>% filter(year == 2020),
aes(log_import)) +
geom_histogram()
# Density plot overlay (import vs export)
us_trade <- us_trade %>%
mutate(
log_export = log(trade_value_usd_exp)
)
us_long <- us_trade %>%
filter(year == 2020) %>%
pivot_longer(
cols = c(log_import, log_export),
names_to = "flow",
values_to = "value"
)
View(us_long)
ggplot(us_long, aes(x = value, fill = flow)) +
geom_density(alpha = 0.5, color = NA, adjust = 1) +
labs(x = "\nLog value of trade",
y = "Density\n",
title = "Density plot overlay (import vs export)",
fill = "Trade flow")
# Who are the top partners? Create:
# Stacked bar: Top 5 partners' share of total 2020 imports
ggplot(top5_partners_2020,
aes(x = "", y = total_imports, fill = partner_name)) +
geom_col(position = "fill") +
scale_y_continuous(labels = scales::percent_format()) +
labs(
x = "2020",
y = "Proportion of total imports\n",
fill = "Partner",
title = "Top 5 partners' share of total 2020 imports"
)
top5_flows_summary <- us_long %>%
filter(
year == 2020,
partner_name %in% top5_partners_2020$partner_name
) %>%
group_by(partner_name, flow) %>%
summarize(
value = sum(value, na.rm = TRUE),
.groups = "drop"
)
ggplot(top5_flows_summary,
aes(x = partner_name, y = value, fill = flow)) +
geom_col(position = "dodge") +
labs(
x = "Partner",
y = "Log trade value",
fill = "Trade flow",
title = "Import vs Export comparison for top 5 partners"
)
# Distribution exploration:
# Boxplot: Import value distribution by top 5 partners
us_trade %>%
filter(
partner_name %in% top5_partners_2020$partner_name
) %>%
ggplot(aes(x = partner_name, y = log_import)) +
geom_boxplot(alpha = 0.7, coef = 1.5, width = 0.4) +
labs(x = "\nPartner", y = "Log import\n")
# Violin + jitter: Same data with individual shipments
us_trade %>%
filter(
partner_name %in% top5_partners_2020$partner_name
) %>%
ggplot(aes(x = partner_name, y = log_import)) +
geom_violin(alpha = 0.7, width = 0.8, draw_quantiles = c(0.25, 0.5, 0.75)) +
geom_jitter(width = 0.15, alpha = 0.2, size = 0.1) +
labs(x = "\nPartner", y = "Log import\n")
# Ridge plot: Partner distributions stacked vertically
us_trade %>%
filter(
partner_name %in% top5_partners_2020$partner_name
) %>%
ggplot(aes(x = log_import, y = partner_name, fill = partner_name)) +
geom_density_ridges(alpha = 0.7, color = "white ", scale = 1.5) +
labs(x = "\nLog import", y = "Partner\n", fill = "Partner")
# Extra questions:
# Which partner shows most growth variability 2018-2020?
##Partner with largest SD of YoY growth across years.
growth_variability <- electronics_growth %>%
filter(!is.na(yoy_growth)) %>%      # remove first year (NA growth)
group_by(partner_name) %>%
summarize(
sd_growth = sd(yoy_growth, na.rm = TRUE),
mean_growth = mean(yoy_growth, na.rm = TRUE),
n_years = n(),
.groups = "drop"
) %>%
arrange(desc(sd_growth))
# Partner with most variability
growth_variability %>% slice_head(n = 1)
# What binwidth best reveals the import value distribution?
##One that: Reveals skew; Avoids spike noise;
##Usually log-scale + wide bins
ggplot(us_trade %>% filter(year == 2020),
aes(log_import)) +
geom_histogram()
electronics_2020 <- electronics %>%
filter(year == 2020,
partner_name %in% top5_partners_2020$partner_name) %>%
group_by(partner_name) %>%
summarize(
total_imports = sum(trade_value_usd_imp, na.rm = TRUE),
.groups = "drop"
)
ggplot(electronics_2020,
aes(x = "", y = total_imports, fill = partner_name)) +
geom_col(position = "fill") +
scale_y_continuous(labels = scales::percent_format()) +
labs(
x = "",
y = "Share of HS85 imports",
fill = "Partner",
title = "Partner share of US electronics imports (2020)"
)
